# Copyright 2022 Intel Corporation
# SPDX-License-Identifier: MIT
#
"""This module implements the connection to the SQLite3 database persisting all benchmarking data generated by AutoSteer"""
import json
import numpy as np
import pandas as pd
import random
import socket
import sys
import os
from datetime import datetime
from sqlalchemy import create_engine, event
from sqlalchemy.sql import text
from sqlalchemy.exc import IntegrityError
import unittest
import sqlite3
from typing import Dict

from utils.custom_logging import logger
from utils.util import read_sql_file

SCHEMA_FILE = 'schema.sql'
ENGINE = None
TESTED_DATABASE = None
BENCHMARK_ID = None
DB_PATH = None
POSTFIX = 0

def init_db(postfix: int):
    """
    main.py 에서 한 번만 호출해서
    storage 모듈 전체에서 쓸 postfix 를 설정
    """
    global POSTFIX
    POSTFIX = postfix

def _get_conn():
    global DB_PATH
    if DB_PATH is None:
        DB_PATH = f"results/{TESTED_DATABASE}_{POSTFIX}.sqlite"
    return sqlite3.connect(DB_PATH)

# Ensure experiment_tags table exists
def _ensure_experiment_tags_table():
    conn = _get_conn()
    conn.execute("""
    CREATE TABLE IF NOT EXISTS experiment_tags (
      benchmark_id INTEGER,
      tag_key      TEXT,
      tag_value    TEXT,
      PRIMARY KEY (benchmark_id, tag_key)
    )""")
    conn.commit()

# Create table at module load
auto_init_conn = _ensure_experiment_tags_table()


def set_experiment_tag(tag_key: str, tag_value: str):
    """
    저장된 BENCHMARK_ID 기준으로 tag_key=tag_value 를 저장/갱신합니다.
    """
    conn = _get_conn()
    conn.execute(
        "INSERT OR REPLACE INTO experiment_tags (benchmark_id, tag_key, tag_value) VALUES (?, ?, ?)",
        (BENCHMARK_ID, tag_key, tag_value)
    )
    conn.commit()


def get_experiment_tags() -> Dict[str, str]:
    """
    현재 BENCHMARK_ID 에 붙은 모든 tag를 {key: value} 형태로 반환합니다.
    """
    conn = _get_conn()
    rows = conn.execute(
        "SELECT tag_key, tag_value FROM experiment_tags WHERE benchmark_id = ?",
        (BENCHMARK_ID,)
    ).fetchall()
    return {k: v for k, v in rows}

def _db():
    global ENGINE
    url = f'sqlite:///results/{TESTED_DATABASE}_{POSTFIX}.sqlite'
    logger.debug('Connect to database: %s', url)
    
    # 데이터베이스 파일이 존재하지 않는 경우에만 새로 생성
    db_path = f'results/{TESTED_DATABASE}_{POSTFIX}.sqlite'
    if not os.path.exists(db_path):
        logger.info(f'Creating new database file: {db_path}')
    
    ENGINE = create_engine(url)

    @event.listens_for(ENGINE, 'connect')
    def connect(dbapi_conn, _):
        """Load SQLite extension for median calculation"""
        try:
            extension_path = './sqlean-extensions/stats.so'
            if not os.path.isfile(extension_path):
                logger.warning('SQLite extension not found at %s, continuing without extension support', extension_path)
                return

            # SQLite extension loading is not supported in all environments
            try:
                dbapi_conn.enable_load_extension(True)
                dbapi_conn.load_extension(extension_path)
                dbapi_conn.enable_load_extension(False)
            except AttributeError:
                logger.warning('SQLite extension loading not supported in this environment, continuing without extension support')
        except Exception as e:
            logger.warning('Failed to load SQLite extension: %s, continuing without extension support', e)

    conn = ENGINE.connect()
    schema = read_sql_file(SCHEMA_FILE)

    for statement in schema.split(';'):
        if len(statement.strip()) > 0:
            try:
                conn.execute(statement)
            except Exception as e:
                print(e)
                raise e
    return conn


def register_benchmark(name: str) -> int:
    # Register a new benchmark and return its id
    with _db() as conn:
        try:
            stmt = text('INSERT INTO benchmarks (name) VALUES (:name)')
            conn.execute(stmt, name=name)
        except IntegrityError:
            pass
        return conn.execute('SELECT benchmarks.id FROM benchmarks WHERE name=:name', name=name).fetchone()[0]


def register_query(query_path):
    # Register a new query
    with _db() as conn:
        try:
            stmt = text('INSERT INTO queries (benchmark_id, query_path, result_fingerprint) VALUES (:benchmark_id, :query_path, :result_fingerprint )')
            conn.execute(stmt, benchmark_id=BENCHMARK_ID, query_path=query_path, result_fingerprint=None)
        except IntegrityError:
            pass


def register_query_fingerprint(query_path, fingerprint):
    with _db() as conn:
        result = conn.execute(text('SELECT result_fingerprint FROM queries WHERE query_path= :query_path'), query_path=query_path).fetchone()[0]
        if result is None:
            conn.execute(text('UPDATE queries SET result_fingerprint = :fingerprint WHERE query_path = :query_path;'),
                         fingerprint=fingerprint, query_path=query_path)
            return True
        elif result != fingerprint:
            return False  # fingerprints do not match
        return True


def register_optimizer(query_path, optimizer, required: bool):
    with _db() as conn:
        try:
            table = 'query_effective_optimizers' if not required else 'query_required_optimizers'
            stmt = text(f'INSERT INTO {table} (query_id, optimizer) '
                        'SELECT id, :optimizer FROM queries WHERE query_path = :query_path')
            conn.execute(stmt, table=table, optimizer=optimizer, query_path=query_path)
        except IntegrityError:
            pass  # do not store duplicates


def register_optimizer_dependency(query_path, optimizer, dependency):
    with _db() as conn:
        try:
            stmt = text('INSERT INTO query_effective_optimizers_dependencies (query_id, optimizer, dependent_optimizer) '
                        'SELECT id, :optimizer, :dependency FROM queries WHERE query_path = :query_path')
            conn.execute(stmt, optimizer=optimizer, dependency=dependency, query_path=query_path)
        except IntegrityError:
            pass  # do not store duplicates


class Measurement:
    """This class stores the measurement for a certain query and optimizer configuration"""

    def __init__(self, query_path, query_id, optimizer_config, disabled_rules, num_disabled_rules, plan_json, walltime):
        self.query_path = query_path
        self.query_id = query_id
        self.optimizer_config = optimizer_config
        self.disabled_rules = disabled_rules
        self.num_disabled_rules = num_disabled_rules
        self.plan_json = json.loads(plan_json)
        self.walltime = walltime


def experience(benchmark=None, cpu=None, IO=None, training_ratio=0.8):
    """Get experience to train a neural network"""
    stmt = """SELECT qu.query_path, q.query_id, q.id,  q.disabled_rules, q.num_disabled_rules, q.query_plan, median(walltime)
            FROM measurements m, query_optimizer_configs q, queries qu
            WHERE m.query_optimizer_config_id = q.id
              AND q.query_plan != 'None' 
              AND qu.id = q.query_id
              AND qu.query_path like :benchmark
              AND m.cpu_load= :cpu
              AND m.io_state = :IO
            group by qu.query_path, q.query_id, q.id, q.disabled_rules, q.num_disabled_rules, q.query_plan"""

    with _db() as conn:
        benchmark = '%%' if benchmark is None else '%%' + benchmark + '%%'
        df = pd.read_sql(stmt, conn, params={'benchmark': benchmark, 'cpu': cpu, 'IO': IO})
    rows = [Measurement(*row) for index, row in df.iterrows()]

    # Group training and test data by query
    result = {}
    for row in rows:
        if row.query_id in result:
            result[row.query_id].append(row)
        else:
            result[row.query_id] = [row]

    keys = list(result.keys())
    # random.seed(41) 
    random.shuffle(keys)
    split_index = int(len(keys) * training_ratio)
    train_keys = keys[:split_index]
    test_keys = keys[split_index:]

    train_data = np.concatenate([result[key] for key in train_keys])
    test_data = np.concatenate([result[key] for key in test_keys])

    return train_data, test_data


def _get_optimizers(table_name, query_path, projections):
    """No SQL injections as this is a private function only called from within *this* module"""
    with _db() as conn:
        stmt = f"""
               SELECT {','.join(projections)}
               FROM queries q, {table_name} qro
               WHERE q.query_path=:query_path AND q.id = qro.query_id AND optimizer != ''
               """
        cursor = conn.execute(stmt, query_path=query_path)
        return cursor.fetchall()


def get_required_optimizers(query_path):
    return list(map(lambda res: res[0], _get_optimizers('query_required_optimizers', query_path, ['optimizer'])))


def get_effective_optimizers(query_path):
    return list(map(lambda res: res[0], _get_optimizers('query_effective_optimizers', query_path, ['optimizer'])))


def get_effective_optimizers_depedencies(query_path):
    return list(map(lambda res: [res[0], res[1]], _get_optimizers('query_effective_optimizers_dependencies', query_path, ['optimizer', 'dependent_optimizer'])))


def get_df(query, params):
    with _db() as conn:
        df = pd.read_sql(query, conn, params=params)
        return df


def select_query(query, params):
    with _db() as conn:
        cursor = conn.execute(query, *params)
        return [row[0] for row in cursor.fetchall()]


def register_query_config(query_path, disabled_rules, query_plan: dict, plan_hash):
    """
    Store the passed query optimizer configuration in the database.
    :returns: query plan is already known and a duplicate
    """
    check_for_duplicated_plans = """SELECT count(*)
        FROM queries q, query_optimizer_configs qoc
        WHERE q.id = qoc.query_id
              AND q.query_path = :query_path
              AND qoc.hash = :plan_hash
              AND qoc.disabled_rules != :disabled_rules"""
    result = select_query(check_for_duplicated_plans, {query_path: query_path, plan_hash: plan_hash, disabled_rules: disabled_rules})
    is_duplicate = result[0] > 0

    with _db() as conn:
        try:
            num_disabled_rules = 0 if disabled_rules is None else disabled_rules.count(',') + 1
            stmt = f"""INSERT INTO query_optimizer_configs
                   (query_id, disabled_rules, query_plan, num_disabled_rules, hash, duplicated_plan) 
                   SELECT id, :disabled_rules, :query_plan_processed , :num_disabled_rules, :plan_hash, :is_duplicate FROM queries WHERE query_path = '{query_path}'
                   """
            conn.execute(stmt, disabled_rules=str(disabled_rules), query_plan_processed=query_plan, num_disabled_rules=num_disabled_rules,
                         plan_hash=plan_hash, is_duplicate=is_duplicate)
        except IntegrityError:
            pass  # OK! Query configuration has already been inserted

    return is_duplicate


def check_for_existing_measurements(query_path, disabled_rules):
    query = """SELECT count(*) as num_measurements
                FROM measurements m, query_optimizer_configs qoc, queries q
                WHERE m.query_optimizer_config_id = qoc.id
                AND qoc.query_id = q.id
                AND q.query_path = :query_path
                AND qoc.disabled_rules = :disabled_rules
             """
    df = get_df(query, {'query_path': query_path, 'disabled_rules': disabled_rules})
    values = df['num_measurements']
    return values[0] > 0


def register_measurement(
    query_path: str,
    disabled_rules: str,
    walltime: int,
    input_data_size: int,
    nodes: int,
    cpu_load: str,
    io_state: str,
):
    """Register a new measurement with CPU load and IO state information"""
    logger.info(
        '새로운 측정값 등록 - 쿼리: %s, I/O 상태: [%s], CPU 부하: [%s], 비활성화된 규칙: [%s]',
        query_path, io_state, cpu_load, disabled_rules
    )
    
    # Validate CPU load value
    valid_cpu_loads = ['LOW', 'MEDIUM', 'HIGH']
    if cpu_load not in valid_cpu_loads:
        logger.warning(f"잘못된 CPU 부하 값: {cpu_load}, 기본값 'LOW' 사용")
        cpu_load = 'LOW'
    
    with _db() as conn:
        now = datetime.now()
        query = """
            INSERT INTO measurements
              (query_optimizer_config_id,
               walltime, machine, time,
               input_data_size, num_compute_nodes,
               cpu_load, io_state)
            SELECT id,
                   :walltime, :host, :time,
                   :input_data_size, :nodes,
                   :cpu_load, :io_state
              FROM query_optimizer_configs
             WHERE query_id = (
                     SELECT id
                       FROM queries
                      WHERE query_path = :query_path
                   )
               AND disabled_rules = :disabled_rules
        """
        try:
            conn.execute(
                query,
                walltime=walltime,
                host=socket.gethostname(),
                time=now.strftime('%m/%d/%y, %H:%M:%S'),
                input_data_size=input_data_size,
                nodes=nodes,
                cpu_load=cpu_load,
                io_state=io_state,
                query_path=query_path,
                disabled_rules=disabled_rules
            )
            logger.info(f"측정값이 성공적으로 저장되었습니다. (CPU 부하: {cpu_load}, 실행 시간: {walltime}μs)")
        except Exception as e:
            logger.error(f"측정값 저장 중 오류 발생: {e}")
            raise

def median_runtimes():
    class OptimizerConfigResult:
        def __init__(self, path, num_disabled_rules, disabled_rules, json_plan, runtime):
            self.path = path
            self.num_disabled_rules = num_disabled_rules
            self.disabled_rules = disabled_rules
            self.json_plan = json_plan
            self.runtime = runtime

    with _db() as conn:
        default_plans_stmt = """SELECT q.query_path, qoc.num_disabled_rules, qoc.disabled_rules, logical_plan_json, elapsed
        FROM queries q,  query_optimizer_configs qoc, measurements m
        WHERE q.id = qoc.query_id AND qoc.id = m.query_optimizer_config_id
        """
        df = pd.read_sql(default_plans_stmt, conn)
        default_median_runtimes = df.groupby(['query_path', 'num_disabled_rules', 'disabled_rules', 'logical_plan_json'])['elapsed'].median().reset_index()

        return [OptimizerConfigResult(*row) for index, row in default_median_runtimes.iterrows()]


def best_alternative_configuration(benchmark=None):
    class OptimizerConfigResult:
        def __init__(self, path, num_disabled_rules, runtime, runtime_baseline, savings, disabled_rules, rank):
            self.path = path
            self.num_disabled_rules = num_disabled_rules
            self.runtime = runtime
            self.runtime_baseline = runtime_baseline
            self.savings = savings
            self.disabled_rules = disabled_rules
            self.rank = rank

    stmt = read_sql_file('best_alternative_queries.sql')

    with _db() as conn:
        cursor = conn.execute(stmt, path=benchmark)
        return [OptimizerConfigResult(*row) for row in cursor.fetchall()]


class TestStorage(unittest.TestCase):
    """Test the storage class"""

    def test_median(self):
        with _db() as db:
            result = db.execute('SELECT MEDIAN(a) FROM (SELECT 1 AS a) AS tab').fetchall()
            assert len(result) == 1

    def test_queries(self):
        with _db() as db:
            result = db.execute('SELECT * FROM queries').fetchall()
            print(result)

    def test_optimizers(self):
        with _db() as db:
            result = db.execute('SELECT * FROM query_effective_optimizers;')
            print(result.fetchall())
